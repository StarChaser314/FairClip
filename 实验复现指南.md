# FairCLIP 实验复现指南

本指南详细说明了如何复现论文《FairCLIP: 视觉语言学习中的公平性利用》中的所有实验。

## 前置准备

### 1. 环境配置
```bash
# 创建conda环境
conda env create -f fairclip.yml
conda activate fairclip

# 或使用uv
uv sync
```

### 2. 数据集准备
确保Harvard-FairVLMed数据集已下载并放置在正确位置：
```
FUNDUS_Dataset/FairVLMed/
├── Training/
├── Validation/
├── Test/
├── data_summary.csv
└── gpt-4_summarized_notes.csv
```

---

## 实验一：主流视觉语言模型的公平性分析

### 1.1 CLIP模型评估

**自然域预训练CLIP (直接评估)**
```bash
cd FairCLIP
./scripts/evaluate_CLIP.sh

```

**医学域微调CLIP (先微调后评估)**
```bash
# 步骤1: 微调CLIP模型
cd FairCLIP
./scripts/finetune_CLIP.sh

# 步骤2: 评估微调后的模型
cd FairCLIP
./scripts/evaluate_CLIP.sh
```

### 1.2 BLIP2模型评估

**自然域预训练BLIP2**
```bash
# 零样本评估
python src/blip_eval.py \
    --cfg-path LAVIS/lavis/projects/blip2/eval/caption_coco_flant5xl_eval.yaml \
    --vision_encoder_weights clip \
    --vl_type blip2 \
    --eval_type zero_shot \
    --prompt "A picture of " \
    --summary_type gpt-4 \
    --data_path /path/to/FUNDUS_Dataset/FairVLMed \
    --output_dir ./results/blip2_natural_zero_shot

# 线性探测评估
cd mae
python -m torch.distributed.launch --master_port=29501 --nproc_per_node=1 main_linprobe.py \
    --model_type blip2 \
    --vl_feats_type image \
    --cfg-path ../LAVIS/lavis/projects/blip2/eval/caption_coco_flant5xl_eval.yaml \
    --vision_encoder_weights clip \
    --batch_size 512 \
    --epochs 1000 \
    --data_path /path/to/FUNDUS_Dataset/FairVLMed \
    --output_dir ../results/blip2_natural_linear_probe
```

**医学域微调BLIP2**
```bash
# 步骤1: 预训练BLIP2
cd LAVIS
python -m torch.distributed.run --nproc_per_node=1 --master_port=29501 train.py \
    --cfg-path lavis/projects/blip2/train/pretrain_stage1.yaml \
    --options datasets.cc_sbu_align.build_info.storage=/path/to/FUNDUS_Dataset/FairVLMed

# 步骤2: 评估微调后的模型
python ../src/blip_eval.py \
    --cfg-path lavis/projects/blip2/train/pretrain_stage1.yaml \
    --weights /home/arch/Codes/FairCLIP/LAVIS/lavis/PRETRAIN_EXPS/Pretrain_Stage1/20250705001/checkpoint_49.pth \
    --vision_encoder_weights clip \
    --prompt "A picture of "\
    --vl_type blip2 \
    --eval_type linear_probe \
    --summary_type gpt-4
```

---

## 实验二：FairCLIP方法有效性验证

### 2.1 零样本迁移实验

**ViT-B/16架构**
```bash
cd FairCLIP

# 训练FairCLIP (种族属性)
./scripts/finetune_FairCLIP_race.sh

# 评估FairCLIP
./scripts/evaluate_CLIP.sh

```

**对其他属性重复实验**


**ViT-L/14架构**
```bash
# 将上述命令中的 --model_arch vit-b16 替换为 --model_arch vit-l14
# 同时调整学习率和批量大小
```


---

## 实验三：消融研究

### 3.1 临床笔记摘要的影响

**GPT-4摘要**
```bash
python src/blip_eval.py \
    --cfg-path LAVIS/lavis/projects/blip2/eval/caption_coco_flant5xl_eval.yaml \
    --vision_encoder_weights clip \
    --vl_type blip2 \
    --eval_type linear_probe \
    --summary_type gpt-4 \
    --data_path /path/to/FUNDUS_Dataset/FairVLMed \
    --output_dir ./results/ablation_summary_gpt4
```

**PMC-LLAMA摘要**
```bash
python src/blip_eval.py \
    --cfg-path LAVIS/lavis/projects/blip2/eval/caption_coco_flant5xl_eval.yaml \
    --vision_encoder_weights clip \
    --vl_type blip2 \
    --eval_type linear_probe \
    --summary_type pmc-llama \
    --data_path /path/to/FUNDUS_Dataset/FairVLMed \
    --output_dir ./results/ablation_summary_pmc_llama
```

**MED42摘要**
```bash
python src/blip_eval.py \
    --cfg-path LAVIS/lavis/projects/blip2/eval/caption_coco_flant5xl_eval.yaml \
    --vision_encoder_weights clip \
    --vl_type blip2 \
    --eval_type linear_probe \
    --summary_type med42 \
    --data_path /path/to/FUNDUS_Dataset/FairVLMed \
    --output_dir ./results/ablation_summary_med42
```

**原始文本（无摘要）**
```bash
python src/blip_eval.py \
    --cfg-path LAVIS/lavis/projects/blip2/eval/caption_coco_flant5xl_eval.yaml \
    --vision_encoder_weights clip \
    --vl_type blip2 \
    --eval_type linear_probe \
    --summary_type original \
    --data_path /path/to/FUNDUS_Dataset/FairVLMed \
    --output_dir ./results/ablation_summary_original
```

### 3.2 视觉特征 vs 多模态特征

**仅视觉特征**
```bash
cd mae
python -m torch.distributed.launch --master_port=29501 --nproc_per_node=1 main_linprobe.py \
    --model_type blip2 \
    --vl_feats_type image \
    --cfg-path ../LAVIS/lavis/projects/blip2/eval/caption_coco_flant5xl_eval.yaml \
    --vision_encoder_weights clip \
    --batch_size 512 \
    --epochs 1000 \
    --data_path /path/to/FUNDUS_Dataset/FairVLMed \
    --output_dir ../results/ablation_vision_only
```

**视觉+语言特征**
```bash
python -m torch.distributed.launch --master_port=29501 --nproc_per_node=1 main_linprobe.py \
    --model_type blip2 \
    --vl_feats_type multimodal \
    --cfg-path ../LAVIS/lavis/projects/blip2/eval/caption_coco_flant5xl_eval.yaml \
    --vision_encoder_weights clip \
    --batch_size 512 \
    --epochs 1000 \
    --data_path /path/to/FUNDUS_Dataset/FairVLMed \
    --output_dir ../results/ablation_multimodal
```

### 3.3 自然域 vs 医学域视觉编码器

**CLIP编码器（自然域）**
```bash
python src/blip_eval.py \
    --cfg-path LAVIS/lavis/projects/blip2/eval/caption_coco_flant5xl_eval.yaml \
    --vision_encoder_weights clip \
    --vl_type blip2 \
    --eval_type linear_probe \
    --summary_type gpt-4 \
    --data_path /path/to/FUNDUS_Dataset/FairVLMed \
    --output_dir ./results/ablation_clip_encoder
```

**PMC-CLIP编码器（医学域）**
```bash
python src/blip_eval.py \
    --cfg-path LAVIS/lavis/projects/blip2/eval/caption_coco_flant5xl_eval.yaml \
    --vision_encoder_weights pmc_clip \
    --vl_type blip2 \
    --eval_type linear_probe \
    --summary_type gpt-4 \
    --data_path /path/to/FUNDUS_Dataset/FairVLMed \
    --output_dir ./results/ablation_pmc_clip_encoder
```

### 3.4 与对抗性公平方法的比较

**标准CLIP**
```bash
cd FairCLIP
python finetune_CLIP.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed \
    --result_dir ./results/comparison_standard_clip \
    --lr 1e-5 \
    --batch_size 32 \
    --epochs 50 \
    --model_arch vit-b16 \
    --summarized_note_file gpt-4_summarized_notes.csv
```

**带对抗性损失的CLIP**
```bash
python finetune_CLIP_adversarial.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed \
    --result_dir ./results/comparison_adversarial_clip \
    --lr 1e-5 \
    --batch_size 32 \
    --epochs 50 \
    --model_arch vit-b16 \
    --attribute race \
    --lambda_adv 1e-3 \
    --summarized_note_file gpt-4_summarized_notes.csv
```

**FairCLIP**
```bash
python finetune_FairCLIP.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed \
    --result_dir ./results/comparison_fairclip \
    --lr 1e-5 \
    --batch_size 32 \
    --epochs 50 \
    --model_arch vit-b16 \
    --attribute race \
    --batchsize_fairloss 32 \
    --lambda_fairloss 1e-6 \
    --sinkhorn_blur 1e-4 \
    --summarized_note_file gpt-4_summarized_notes.csv
```

### 3.5 FairCLIP超参数影响

**不同批量大小影响**
```bash
# |Ba| = 16
python finetune_FairCLIP.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed \
    --result_dir ./results/ablation_batch_16 \
    --lr 1e-5 \
    --batch_size 32 \
    --epochs 50 \
    --model_arch vit-b16 \
    --attribute race \
    --batchsize_fairloss 16 \
    --lambda_fairloss 1e-6 \
    --sinkhorn_blur 1e-4 \
    --summarized_note_file gpt-4_summarized_notes.csv

# |Ba| = 32
python finetune_FairCLIP.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed \
    --result_dir ./results/ablation_batch_32 \
    --lr 1e-5 \
    --batch_size 32 \
    --epochs 50 \
    --model_arch vit-b16 \
    --attribute race \
    --batchsize_fairloss 32 \
    --lambda_fairloss 1e-6 \
    --sinkhorn_blur 1e-4 \
    --summarized_note_file gpt-4_summarized_notes.csv

# |Ba| = 64
python finetune_FairCLIP.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed \
    --result_dir ./results/ablation_batch_64 \
    --lr 1e-5 \
    --batch_size 64 \
    --epochs 50 \
    --model_arch vit-b16 \
    --attribute race \
    --batchsize_fairloss 64 \
    --lambda_fairloss 1e-6 \
    --sinkhorn_blur 1e-4 \
    --summarized_note_file gpt-4_summarized_notes.csv
```

**不同正则化参数ε影响**
```bash
# ε = 1e-3
python finetune_FairCLIP.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed \
    --result_dir ./results/ablation_epsilon_1e-3 \
    --lr 1e-5 \
    --batch_size 32 \
    --epochs 50 \
    --model_arch vit-b16 \
    --attribute race \
    --batchsize_fairloss 32 \
    --lambda_fairloss 1e-6 \
    --sinkhorn_blur 1e-3 \
    --summarized_note_file gpt-4_summarized_notes.csv

# ε = 1e-4
python finetune_FairCLIP.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed \
    --result_dir ./results/ablation_epsilon_1e-4 \
    --lr 1e-5 \
    --batch_size 32 \
    --epochs 50 \
    --model_arch vit-b16 \
    --attribute race \
    --batchsize_fairloss 32 \
    --lambda_fairloss 1e-6 \
    --sinkhorn_blur 1e-4 \
    --summarized_note_file gpt-4_summarized_notes.csv

# ε = 1e-5
python finetune_FairCLIP.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed \
    --result_dir ./results/ablation_epsilon_1e-5 \
    --lr 1e-5 \
    --batch_size 32 \
    --epochs 50 \
    --model_arch vit-b16 \
    --attribute race \
    --batchsize_fairloss 32 \
    --lambda_fairloss 1e-6 \
    --sinkhorn_blur 1e-5 \
    --summarized_note_file gpt-4_summarized_notes.csv
```

---

## 注意事项

1. **路径替换**：请将所有 `/path/to/FUNDUS_Dataset/FairVLMed` 替换为实际的数据集路径。

2. **计算资源**：某些实验（特别是ViT-L/14和BLIP2相关）需要大量GPU内存，建议使用高性能GPU。

3. **并行训练**：对于BLIP2实验，可以根据可用GPU数量调整 `--nproc_per_node` 参数。

4. **结果保存**：所有实验结果将保存在各自的 `result_dir` 中，包括模型权重、日志和评估指标。

5. **批量运行**：可以创建bash脚本批量运行多个实验，建议按实验类型分组运行。

6. **数据预处理**：确保在运行实验前，所有必要的数据预处理（如GPT-4摘要生成）已完成。