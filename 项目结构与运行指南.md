# FairCLIP 项目结构与运行指南

## 项目概述

FairCLIP 是一个专注于医学影像中视觉-语言学习公平性的研究项目。项目引入了首个公平视觉-语言医学数据集（Harvard-FairVLMed），并提出了基于最优传输的 FairCLIP 方法，用于减少 CLIP 模型在受保护属性（种族、性别、民族、语言）上的偏见。

## 项目文件结构

```
FairCLIP/
├── FUNDUS_Dataset/FairVLMed/          # 数据集目录
│   ├── Training/                      # 训练数据 (.npz 文件)
│   ├── Validation/                    # 验证数据
│   ├── Test/                         # 测试数据
│   ├── data_summary.csv              # 数据摘要
│   └── gpt-4_summarized_notes.csv    # GPT-4 生成的摘要笔记
│
├── FairCLIP/                         # 核心 FairCLIP 实现
│   ├── src/
│   │   ├── modules.py                # 核心工具模块
│   │   ├── logger.py                 # 日志工具
│   │   └── generate_split_file.py    # 数据分割工具
│   ├── scripts/                      # 运行脚本
│   │   ├── finetune_FairCLIP.sh      # FairCLIP 训练脚本
│   │   ├── finetune_CLIP.sh          # CLIP 训练脚本
│   │   └── evaluate_CLIP.sh          # 评估脚本
│   ├── finetune_FairCLIP.py          # FairCLIP 训练主程序
│   ├── finetune_CLIP.py              # CLIP 训练主程序
│   ├── evaluate_CLIP.py              # CLIP 评估主程序
│   ├── requirements.txt              # Python 依赖
│   └── README.md
│
├── LAVIS/                            # BLIP2 框架集成
│   ├── lavis/                        # LAVIS 核心库
│   │   ├── projects/blip2/           # BLIP2 项目配置
│   │   ├── models/                   # 模型定义
│   │   ├── datasets/                 # 数据集构建器
│   │   └── processors/               # 数据预处理器
│   ├── train.py                      # BLIP2 训练主程序
│   ├── evaluate.py                   # BLIP2 评估程序
│   └── requirements.txt
│
├── mae/                              # MAE (Masked AutoEncoder) 集成
│   ├── main_linprobe.py              # 线性探测主程序
│   ├── main_pretrain.py              # 预训练主程序
│   ├── main_finetune.py              # 微调主程序
│   ├── models_mae.py                 # MAE 模型定义
│   ├── models_vit.py                 # Vision Transformer 模型
│   └── util/                         # 工具函数
│
├── moco-v3/                          # MoCo v3 实现
│   ├── main_moco.py                  # MoCo 预训练
│   ├── main_lincls.py                # 线性分类
│   └── moco/                         # MoCo 核心模块
│
├── src/                              # 共享评估脚本
│   ├── blip_eval.py                  # BLIP 模型评估
│   ├── clip_eval.py                  # CLIP 模型评估
│   ├── fundus_dataloader.py          # 眼底数据加载器
│   └── dataset_deidentification_summarization.py  # 数据脱敏和摘要
│
├── summarize/                        # 摘要生成模块
│   └── summarizer_models.py          # 摘要模型
│
├── fairclip.yml                      # Conda 环境配置
├── pyproject.toml                    # uv 项目配置
└── main.py                          # 项目入口
```

## 环境配置

### 方法一：使用 Conda
```bash
# 创建并激活环境
conda env create -f fairclip.yml
conda activate fairclip
```

### 方法二：使用 uv（推荐）
```bash
# 安装依赖
uv sync

# 激活虚拟环境
source .venv/bin/activate
```

### PyTorch 版本升级（可选）
```bash
# 升级到更新的 PyTorch 版本
pip install torch==2.2.2+cu121 -f https://download.pytorch.org/whl/torch_stable.html
```

## 数据集准备

1. **下载数据集**：从提供的 Google Drive 链接下载 Harvard-FairVLMed 数据集
2. **目录结构**：确保数据集按以下结构放置：
```
FUNDUS_Dataset/FairVLMed/
├── Training/        # 包含 data_xxxxx.npz 文件
├── Validation/      # 包含 data_xxxxx.npz 文件
├── Test/           # 包含 data_xxxxx.npz 文件
├── data_summary.csv
└── gpt-4_summarized_notes.csv
```

## 模型训练与评估

### 1. CLIP 模型

#### 训练标准 CLIP
```bash
cd FairCLIP

# 基础训练命令
python finetune_CLIP.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed \
    --result_dir ./results/glaucoma_CLIP_vit-b16 \
    --lr 1e-5 \
    --batch_size 32 \
    --model_arch vit-b16 \
    --num_epochs 10 \
    --summarized_note_file gpt-4_summarized_notes.csv

# 使用脚本运行
bash scripts/finetune_CLIP.sh
```

#### 评估 CLIP 模型
```bash
python evaluate_CLIP.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed \
    --result_dir ./results/glaucoma_CLIP_vit-b16 \
    --model_arch vit-b16 \
    --pretrained_weights ./results/checkpoint/clip_ep002.pth

# 使用脚本运行
bash scripts/evaluate_CLIP.sh
```

### 2. FairCLIP 模型

#### 训练 FairCLIP（公平性约束）
```bash
cd FairCLIP

# 针对种族属性的公平训练
python finetune_FairCLIP.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed \
    --result_dir ./results/glaucoma_FairCLIP_vit-b16_race \
    --lr 1e-5 \
    --batch_size 32 \
    --model_arch vit-b16 \
    --attribute race \
    --batchsize_fairloss 32 \
    --lambda_fairloss 1e-6 \
    --sinkhorn_blur 1e-4 \
    --num_epochs 10 \
    --summarized_note_file gpt-4_summarized_notes.csv

# 针对其他属性（性别、民族、语言）
python finetune_FairCLIP.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed \
    --result_dir ./results/glaucoma_FairCLIP_vit-b16_gender \
    --attribute gender \
    [其他参数相同...]

# 使用脚本运行
bash scripts/finetune_FairCLIP.sh
```

#### FairCLIP 关键参数说明
- `--attribute`: 受保护属性 (`race`, `gender`, `ethnicity`, `language`)
- `--lambda_fairloss`: 公平性损失权重 (推荐: 1e-6)
- `--batchsize_fairloss`: 公平性损失的批次大小
- `--sinkhorn_blur`: Sinkhorn 算法的模糊参数

### 3. BLIP2 模型

#### 预训练 BLIP2
```bash
cd LAVIS

# 第一阶段预训练
python -m torch.distributed.run \
    --nproc_per_node=1 \
    --master_port=29501 \
    train.py \
    --cfg-path lavis/projects/blip2/train/pretrain_stage1.yaml

# 第二阶段预训练
python -m torch.distributed.run \
    --nproc_per_node=1 \
    --master_port=29502 \
    train.py \
    --cfg-path lavis/projects/blip2/train/pretrain_stage2.yaml
```

#### 评估 BLIP2
```bash
# 零样本评估
python src/blip_eval.py \
    --cfg-path /path/to/config.yaml \
    --weights /path/to/checkpoint.pth \
    --vision_encoder_weights clip \
    --vl_type blip2 \
    --eval_type zero_shot \
    --prompt \"A picture of \" \
    --summary_type gpt-4

# 线性探测评估
python src/blip_eval.py \
    --cfg-path /path/to/config.yaml \
    --weights /path/to/checkpoint.pth \
    --vision_encoder_weights clip \
    --vl_type blip2 \
    --eval_type linear_probe \
    --summary_type gpt-4
```

### 4. 线性探测（Linear Probing）

#### 使用 MAE 进行线性探测
```bash
cd mae

python -m torch.distributed.launch \
    --master_port=29501 \
    --nproc_per_node=1 \
    main_linprobe.py \
    --model_type blip2 \
    --vl_feats_type image \
    --cfg-path ../LAVIS/lavis/projects/blip2/train/pretrain_stage1.yaml \
    --vision_encoder_weights clip \
    --batch_size 512 \
    --epochs 1000 \
    --data_path /path/to/FUNDUS_Dataset/FairVLMed \
    --nb_classes 2 \
    --output_dir ./output_linprobe
```

## 模型架构选项

### CLIP 架构
- `vit-b16`: Vision Transformer Base 16x16 patches
- `vit-l14`: Vision Transformer Large 14x14 patches

### 评估类型
- `zero_shot`: 零样本评估
- `linear_probe`: 线性探测评估

### 摘要类型
- `original`: 原始临床笔记
- `gpt-4`: GPT-4 生成的摘要
- `gpt-3.5-turbo`: GPT-3.5 生成的摘要
- `pmc-llama`: PMC-LLaMA 生成的摘要
- `med42`: Med42 生成的摘要

## 公平性评估指标

项目实现了多种公平性评估指标：

1. **人口统计奇偶性差异 (DPD)**: Demographic Parity Difference
2. **机会均等差异 (EOD)**: Equalized Odds Difference
3. **权益缩放准确率 (ES-ACC)**: Equity-Scaled Accuracy
4. **权益缩放 AUC (ES-AUC)**: Equity-Scaled AUC
5. **组间差异**: Between-group disparity metrics

## 数据预处理

### 生成 LLM 摘要
```bash
python src/dataset_deidentification_summarization.py \
    --openai_key <YOUR_OPENAI_KEY> \
    --models gpt-4
```

### 数据分割
```bash
cd FairCLIP/src
python generate_split_file.py \
    --dataset_dir /path/to/FUNDUS_Dataset/FairVLMed
```

## 结果分析

训练和评估结果将保存在指定的 `result_dir` 中，包括：
- 模型检查点 (`.pth` 文件)
- 性能指标 (`.csv` 文件)
- 训练日志
- 配置参数

### 典型输出文件
```
results/glaucoma_FairCLIP_vit-b16_race/
├── checkpoint/
│   ├── clip_ep001.pth
│   ├── clip_ep002.pth
│   └── ...
├── args_train.txt           # 训练参数
├── train.log               # 训练日志
└── performance_metrics.csv # 性能指标
```

## 注意事项

1. **计算资源**: 训练视觉-语言模型需要大量计算资源，建议使用 GPU
2. **数据集许可**: 数据集使用需遵守 CC BY-NC-ND 4.0 许可协议
3. **路径配置**: 脚本中的路径为占位符，需要根据实际数据集位置更新
4. **CUDA 兼容性**: 请根据系统配置验证 CUDA 兼容性

## 常见问题排解

1. **内存不足**: 减小 `batch_size` 参数
2. **CUDA 错误**: 检查 PyTorch 和 CUDA 版本兼容性
3. **数据加载错误**: 验证数据集路径和文件结构
4. **依赖冲突**: 使用提供的环境配置文件重新创建环境

## 参考资源

- OpenAI CLIP: https://github.com/openai/CLIP
- LAVIS (BLIP2): https://github.com/salesforce/LAVIS
- Fairlearn: https://fairlearn.org/
- 论文相关代码和数据集链接详见原始仓库