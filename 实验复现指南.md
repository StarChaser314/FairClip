# FairCLIP 实验复现指南

本指南详细说明了如何复现论文《FairCLIP: 视觉语言学习中的公平性利用》中的所有实验。

## 前置准备

### 1. 环境配置
```bash
# 创建conda环境
conda env create -f fairclip.yml
conda activate fairclip

# 或使用uv
uv sync
```

### 2. 数据集准备
确保Harvard-FairVLMed数据集已下载并放置在正确位置：
```
FUNDUS_Dataset/FairVLMed/
├── Training/
├── Validation/
├── Test/
├── data_summary.csv
└── gpt-4_summarized_notes.csv
```

---

## 实验一：主流视觉语言模型的公平性分析

### 1.1 CLIP模型评估

**自然域预训练CLIP (直接评估)**
```bash
cd FairCLIP
./scripts/evaluate_CLIP.sh

```

**医学域微调CLIP (先微调后评估)**
```bash
# 步骤1: 微调CLIP模型
cd FairCLIP
./scripts/finetune_CLIP.sh

# 步骤2: 评估微调后的模型
cd FairCLIP
./scripts/evaluate_CLIP.sh
```

### 1.2 BLIP2模型评估

**医学域微调BLIP2**
```bash
# 步骤1: 预训练BLIP2
cd LAVIS
python -m torch.distributed.run --nproc_per_node=1 --master_port=29501 train.py --cfg-path lavis/projects/blip2/train/pretrain_stage1.yaml

# 步骤2: 评估微调后的模型 线性探测
cd mae
./evaluate.sh
```

---

## 实验二：FairCLIP方法有效性验证

### 2.1 零样本迁移实验

**ViT-B/16架构**
```bash
cd FairCLIP

# 训练FairCLIP (种族属性)
./scripts/finetune_FairCLIP_race.sh

# 评估FairCLIP
./scripts/evaluate_CLIP.sh

```

**对其他属性重复实验**


**ViT-L/14架构**
```bash
# 将上述命令中的 --model_arch vit-b16 替换为 --model_arch vit-l14
# 同时调整学习率和批量大小
```


---

## 实验三：消融研究

### 3.2 视觉特征 vs 多模态特征

**仅视觉特征**


**视觉+语言特征**

对finetune的模型，在evaluate里面改type


### 3.3 自然域 vs 医学域视觉编码器

**CLIP编码器（自然域）**

**PMC-CLIP编码器（医学域）**

对finetune的模型，在evaluate里面改encoder

### 3.4 FairCLIP超参数影响

在finetune fairclip里面改
